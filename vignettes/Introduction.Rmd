---
title: "dioscRi package manual"
author: "Elijah Willie"
date: "`r Sys.Date()`"
output:
    BiocStyle::html_document:
        toc: true
vignette: >
  %\VignetteIndexEntry{dioscRi package manual}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r knitr-options, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
# opts_chunk$set(fig.align = 'center', fig.width = 6, fig.height = 5, dev = 'png')
```

# Installation
```{r, eval = FALSE}
# install.packages("devtools")
devtools::install_github("https://github.com/ecool50/dioscRi/")
library(dioscRi)
```

# Introduction

`dioscRi` is an R package designed to provide an end-to-end solution for identifying significant features in single-cell or multiplexed imaging data. Leveraging hierarchical clustering, overlapping group lasso, and downstream interpretative tools, dioscRi allows researchers to identify and understand the biological relevance of features, offering a robust framework for hypothesis generation and data-driven insights.

# Overview of the dioscRi workflow

The dioscRi pipeline comprises several distinct yet interconnected steps, ensuring a smooth analytical workflow for feature identification and downstream interpretation:

<img src=https://raw.githubusercontent.com/ecool50/dioscRi/main/inst/dioscRi_overview.png align="middle" height="400" width="1200">

## Summary of the **dioscRi** Pipeline

- **Transferable Normalization**:
  - A neural network is trained on the training samples to remove technical variability while preserving biological heterogeneity.
  - This step ensures consistent data normalization for downstream analysis.

- **Cell Typing**:
  - Cell populations are identified using clustering, manual gating, or reference-based methods.
  - This step establishes distinct cell type labels for each sample.

- **Cell Type Grouping**:
  - Hierarchical relationships between cell types are defined using tools like **treekoR** or through manual annotations.
  - Groups are structured for interpretability and predictive modeling.

- **Prediction and Interpretation**:
  - Hierarchical group lasso models are used to make predictions based on grouped cell types or features.
  - The model provides interpretable outputs to understand key contributing features or cell populations.

- **New Sample Normalization**:
  - The trained normalization model is applied to normalize new samples, ensuring compatibility with the training data.

- **Cell Type Classification**:
  - Cell types in new samples are classified based on the pre-defined cell type structure and trained model.

- **New Sample Classification**:
  - Patient-level predictions are generated for new samples, providing probabilities or classifications (e.g., healthy vs. diseased).
  - This final step enables the application of the model for clinical or experimental outcomes.
  
- **Notes**:
  - Due to the random nature of deep learning models, the results presented here will most likely differ from what is presented in the manuscript.
  - Some of the packages used at the time of development may have new versions that may change some computations.
    - This is most relevant for `Keras` and `Tensorflow`
  - For the fully reproducible version, the saved `sce` objects are provided in anothe vignette.
  
# BioHEART-CT - Background
The **BioHEART-CT** cohort study, a prospective longitudinal study, profiled immune cell populations associated with coronary artery disease (CAD) using mass cytometry. The dataset includes discovery (111 samples, 6 batches) and validation cohorts (58 samples, 3 batches), with manual gating identifying 11 major and 81 minor cell populations. CAD severity was measured using the Gensini score, dichotomized into CAD+ (Gensini > 0) and CAD- (Gensini = 0). Predictive modeling on this dataset highlighted the association of T regulatory cells expressing markers like CCR2, CCR4, CD39, and HLA-DR with CAD.

In this demonstration, we will utilize the dioscRi framework to model CAD status on this dataset, showcasing its ability to construct interpretable predictive models for high-dimensional cytometry data.

# Load in required libraries
First we need to load in the required libraries for the models we will be fitting
```{r}
rm(list = ls())
gc()
suppressPackageStartupMessages({
    library(dioscRi)
    library(keras3)
    library(tidyverse)
    library(tensorflow)
    library(data.table)
    library(caret)
    library(SingleCellExperiment)
    library(FuseSOM)
})

set_random_seed(1994)
```

# Set the markers
We will be using a subset of 27 markers used in the original manuscript for manual gating. 
```{r}
useMarkers <- c('HLA_DR', 'CD3', 'CD4', 'CD8a', 'CD25', 'CD127', 'FoxP3', 'CD27',
                'KLRG1', 'CD56', 'CD45RO', 'CD45RA', 'CD192_CCR2', 'CD194_CCR4',
                'CD196_CCR6',
                'CD39', 'CD38', 'Ki67', 'CD183_CXCR3', 'CCR7', 'CD19', 'CD20',
                'IgD', 'CD14', 'CD304', 'CD141', 'CD1c_PE')

allMarkers <- c('HLA_DR', 'CD3', 'CD4', 'CD8a', 'CD25', 'CD127', 'FoxP3', 'CD27',
                'KLRG1', 'CD56', 'CD45RO', 'CD45RA', 'CD192_CCR2', 'CD194_CCR4',
                'CD196_CCR6',
                'CD39', 'CD38', 'Ki67', 'CD183_CXCR3', 'CCR7', 'CD19', 'CD20',
                'IgD', 'CD14', 'CD304', 'CD141', 'CD1c_PE', "CD11b", "CD253_TRAIL",
                "CD34", "CD61", "CD11c", "eNOS", "LOX_1", "CD86", "CD16", "CD45_106",
                "CD45_108", "P2X7", "NOX5")
```


# Read in the datasets
The discovery study will be used to for training and the validation study will be used for testing
```{r}
# Discovery study
set.seed(1994)
df_train <- fread('../../raw_data/bioheart_ct_cytof_data_b4_mg.csv',
              nThread = 7) %>%
  as.data.frame()

# Validation study
df_test <- fread('../../Study_3_2019/all_batches_processed_mg_10K.csv',
              nThread = 7) %>% 
  as.data.frame()
```


# Dataset transformation
As is standard with cytometry data, the `arsinh` transformation will be applied.
The arcsinh transformation is commonly used in cytometry data to manage its wide dynamic range and skewed distribution. It compresses large signal intensities while preserving low-intensity values and handles negative or near-zero values effectively, unlike logarithmic transformations. This transformation improves statistical analyses, such as clustering, and enhances visualization by clearly separating cell populations, ensuring consistency and interpretability across studies.
```{r}
# the training data
df_train[, useMarkers]  <- cyCombine::transform_asinh(df_train[, useMarkers], 
                                                markers = useMarkers, derand = F)

colnames(df_train)[colnames(df_train) %in% useMarkers] <- 
  gsub("_", "-", colnames(df_train)[colnames(df_train) %in% useMarkers])

# the testing data
df_test[, useMarkers]  <- cyCombine::transform_asinh(df_test[, useMarkers],
                                                  markers = useMarkers, derand = F)
colnames(df_test)[colnames(df_test) %in% useMarkers] <- 
  gsub("_", "-", colnames(df_test)[colnames(df_test) %in% useMarkers])

useMarkers <- gsub("_", "-", useMarkers)
allMarkers <- gsub("_", "-", allMarkers)
```


# Range transformation for the MMD-VAE model
Before fitting the MMD-VAE deep learning model, we need to transform the range of the dataset to [0,1]. Doing so stabilises the model and speeds up convergence. To do this, a range transformer is trained using the training data and then applied to both the training and the testing data.  
```{r}
# train range transformer on the training data using the caret package
preProcValues <- preProcess(df_train[, useMarkers], method = c("range"))

# predict on the training data
df_train[, useMarkers] <- predict(preProcValues, df_train[, useMarkers])

# predict on the testing data
df_test[, useMarkers] <- predict(preProcValues, df_test[, useMarkers])
```
 
# Get the reference samples for training and validating the MMD-VAE model
To further speed up training of the MMD-VAE model, we select a set of $k$ samples which will be used for training. To select these samples, we firt compute the covariance matrix for each sample, next the pairwise frobenius norms of the covariances for all samples are computed. Finally the top $k$ samples are selected by averaging all the frobenious norms for each sample and returning the top $k$ samples with the minimum average. We set $k$ to $2$ in our analyses. 

```{r}
# compute the reference samples
res_ind <- computeReferenceSample(df_train, useMarkers, N = 2)

# get the training samples
train_dat <- df_train[which(df_train$sample_id %in% c(res_ind$topNSamples)), ]

# get the validation samples
val_dat <- df_train[which(df_train$sample_id %in% c(res_ind$bottomNSamples)), ]
```


# Fit the MMD-VAE model
For training a transferable normalisation model, we use the Maximum Mean Discrepancy Variational Autoencoder (MMD-VAE) which uses the MMD loss instead of the KL-divergence loss. For added regularisation, we set $\lambda = 0.01$. The model is fit with batch size of $32$ over $100$ epochs.
```{r}
batch_size = 32L
vae_model <- trainVAEModel(trainData = train_dat[, useMarkers], batchSize = batch_size, 
                             useMarkers = useMarkers, epochs = 100, lambda = 0.01,
                             valData = val_dat[, useMarkers])
```

# Normalise both the training and testing using the fitted VAE model
After training the normalisation model, both the training and the testind data are normalised using the trained model
```{r}
train_decoded <- decodeSamples(newSamples = as.matrix(df_train[, useMarkers]), 
                                vae = vae_model$vae)
df_norm <- train_decoded[[1]]

colnames(df_norm) <- useMarkers


test_decoded <- decodeSamples(newSamples = as.matrix(df_test[, useMarkers]), 
                               vae = vae_model$vae)
df_test_norm <- test_decoded[[1]]

colnames(df_test_norm) <- useMarkers
```


# Create SCE objects
Before performing downstream analyses, we first compile both the training and testing datasets into a `SingleCellExperiment`(SCE) object. We also add the embeddings from the vae model into the SCE object for later use. 
```{r}
# the training data
sce_train <- SingleCellExperiment(assays = list(norm = t(df_norm[, useMarkers])
),
colData = df_train %>% dplyr::select(-useMarkers))
## add the vae embeddings
reducedDims(sce_train) <- list(VAE=train_decoded$encoded %>%
                                as.matrix() %>%
                                as.data.frame())

# testing data
sce_test <- SingleCellExperiment(assays = list(norm = t(df_test_norm[, useMarkers])
),
colData = df_test %>% dplyr::select(-useMarkers))
## add the vae embeddings
reducedDims(sce_test) <- list(VAE=test_decoded$encoded %>%
                                as.matrix() %>%
                                as.data.frame())
```


# Cluster the training data
Next, the normalised training data is clustered into $k=11$ clusters using the `FuseSOM` algorithm. The `FuseSOM` algorithm combines multiple similarity metrics through multiview ensemble learning and hierarchical clustering to create cell types.
```{r, eval=T}
sce_train$clusters_norm <- NULL
nclust <- 11
sce_train <- runFuseSOM(sce_train, numClusters = nclust, assay = 'norm', 
                       verbose = FALSE, clusterCol = 'clusters_norm')
```

```{r, eval=T}
aricode::ARI(sce_train$clusters_norm, sce_train$mg_cell_type_distinct)
```

# Train cell type identification model
Before doing patient classification, we need to compute the clusters for the testing data. To do so, we use the clusters from the training data to train a `Linear Discriminant Analysis` (LDA) model using the vae embeddings from earlier. The `caret` package is used to fit this mode
```{r}
# set up the training data
train_x <- reducedDim(sce_train, type = "VAE") %>%
  mutate(cellTypes = as.factor(sce_train$clusters_norm))

# setup th testing data
test_x <- reducedDim(sce_test, type = "VAE")

# train the model
df_test_clusters <- trainCellTypeClassifier(trainX = train_x, testX = test_x, model = 'lda')

# update the clusters for the test data
sce_test$clusters_norm <- df_test_clusters
```


# Set up clinical information 
Next, we setup the clinical information that will be used in the patient prediction model. For the BioHEART-CT dataset, we are predicting `CAD` status via `Gensini` which is a continous variable. In order to fit the prediction model, we binarise it to $Gensini > 0$ and $Gensini = 0$.
```{r}
# The training data
clinicaldata_train <- colData(sce_train) %>%
  as.data.frame() %>%
  dplyr::select(sample_id, Gensini, Gender, Age) %>%
  distinct()
# binarise the gensini variable
clinicaldata_train$Gensini_bin <- factor(if_else(clinicaldata_train$Gensini > 0, 1, 0))

# The testing data - gensini is already binarised
clinicaldata_test <- colData(sce_test) %>%
  as.data.frame() %>%
  dplyr::select(sample_id, Gensini_bin) %>%
  distinct()
```


# Compute feature sets
Next, the logit of cell type proportions in each sample and the marker means per cell type in each sample are computed. Both of these features with be used to train a model predict `CAD` status.
## Training data
```{r}
# compute logit of proportions for the training set
prop_logit_train <- computeFeatures(sce = sce_train, featureType = 'prop', 
                              cellTypeCol = 'clusters_norm', sampleCol = 'sample_id', 
                              logit = T, useMarkers = useMarkers)

# compute the marker means per cell type for the training set
markerMeanCellType_train <- computeFeatures(sce = sce_train, featureType = 'mean', 
                              cellTypeCol = 'clusters_norm', sampleCol = 'sample_id', 
                              logit = T, useMarkers = useMarkers)

# setup response as well
row_names_train <- rownames(prop_logit_train)
y_train <- factor(clinicaldata_train[match(row_names_train,clinicaldata_train$sample_id),
                                      "Gensini_bin"])
y_train <- as.numeric(levels(y_train))[y_train]
```

## Testing data
```{r}
# compute logit of proportions for the testing set
prop_logit_test <- computeFeatures(sce = sce_test, featureType = 'prop', 
                              cellTypeCol = 'clusters_norm', sampleCol = 'sample_id', 
                              logit = T, useMarkers = useMarkers)

# compute the marker means per cell type for the testing set
markerMeanCellType_test <- computeFeatures(sce = sce_test, featureType = 'mean', 
                              cellTypeCol = 'clusters_norm', sampleCol = 'sample_id', 
                              logit = T, useMarkers = useMarkers)

# setup response as well
row_names_test <- rownames(prop_logit_test)
y_test <- factor(clinicaldata_test[match(row_names_test,clinicaldata_test$sample_id),
                                      "Gensini_bin"])
y_test <- as.numeric(levels(y_test))[y_test]
```


# Generate grouping structures
The final input to overlapping group lasso model is the hierarchical structure of the cell type proportions . To generate this, we run hierarchical clustering on the logit proportions using ward linkage function on the correlation distances. The resulting tree structure is then traversed and all possible groupings of cells types in the tree are turned. This is then combined with the marker means per cell type. Note that each element in the marker means per cell type is in a group 1.
```{r}
trees <- generateTree(features = prop_logit_train, method = "ward")
tree <- trees$tree
order <- trees$order
groups <- generateGroups(tree = tree, nClust = nclust, 
                         proportions = prop_logit_train, means = markerMeanCellType_train)
head(groups, 30)
```

# Process training and testing data
Now that we have the features and the grouping structure generated, we can combine the features into a matrix before passing it to overlapping group lasso model for training.
## Training data
```{r}
# Combine logit proporitons and marker means per cell type
X_train <- cbind(prop_logit_train, markerMeanCellType_train) %>% 
  as.matrix()
  
# Scale the combined data so coefficients are comparable
scaleVals <- preProcess(X_train, method = c('scale'))
X_train <- predict(scaleVals, X_train) %>%
    as.matrix()
```

## Testing data
```{r}
# Combine logit proporitons and marker means per cell type
X_test <- cbind(prop_logit_test, markerMeanCellType_test) %>%
  as.matrix()

# scale using scaling model from training set
X_test <- predict(scaleVals, X_test) %>%
    as.matrix()
```

# Fit Overlap model
Finally, everything is ready for us to fit the prediction model. The model used is the overlapping group lasso that is described [here](https://pbreheny.github.io/grpreg/index.html). The model was originally designed for group based lasso. However, we have modified it to be applied to overlapping groups based on [here](https://pmc.ncbi.nlm.nih.gov/articles/PMC5026200/). To fit the model, we pass the features, response, and grouping structure. As with the standard lasso model, we optimize the values of $\alpha$ and $\lambda$ simultaneously using the `Deviance` criterion. For our models, we generate $10$ values between $[0,1]$ for $\alpha$ and $1000$ values between $[0.05, 1]$ for $\lambda$ the optimal value for both parameters is selected using `elbow` method on the deviances.
```{r}
# fit the model
fit <- fitModel(xTrain = X_train, yTrain = y_train, groups = groups, 
                penalty = 'grLasso', seed = 1994)
```

# Assess model performace
To assess how well the model is performing, we use the ROC-AUC curve.

## Training AUC {.tabset}
```{r}
train_auc <- plotAUC(fit = fit$fit, xTest = X_train, yTest = y_train, title = "Study 3 - All AUC =")
train_auc$plot
```

## Testing AUC {.tabset}
```{r}
test_auc <- plotAUC(fit = fit$fit, xTest = X_test, yTest = y_test, title = "Study 3 - All AUC =")
test_auc$plot

```


# Visualise and assess  model outputs
We can look at which features are driving my model's performance by looking at the feature tree which combines the hierarchy of the cell type proportions with marker means per cell type. For the hierarchy, the color of the node represents the value of the coefficient in the final model.
```{r, fig.height=10, fig.width=12}
visualiseModelTree(fit = fit$fit, tree = tree, type = "cluster",
                   trainingData = X_train, title = "Unsupervised Clustering")
```

Next we can look at some distribution plots for the features with non-zero coeffients in the model to see how they differ between both classes. We can also assess their statistical significance.

## Means {.tabset}
```{r, eval=T}
# CCR2 (OR 1.12), CCR4 (OR 1.08), CD38 and CD45RO (OR 1.13), HLA-DR (OR 1.06), and Ki67 (1.22)
stats_mean <- getSigFeatures(fit$fit, type = 'mean',  
                         mean = markerMeanCellType_train, clinicalData = clinicaldata_train, 
                         outcome = "Gensini_bin")

sigFeatures <- stats_mean$sigFeatures
stats <- stats_mean$stats

```

```{r, fig.height=10, fig.width=16}
plotSigFeatures(stats = stats, sigFeatures = sigFeatures, outcome = "Gensini bin")

plotSigFeatures(stats = stats, sigFeatures = sigFeatures, outcome = "Gensini bin", 
                type = "boxplots") + 
    xlab("Gensini bin")
```

## Proportions {.tabset}
```{r, eval=T, fig.height=10, fig.width=16}
# CCR2 (OR 1.12), CCR4 (OR 1.08), CD38 and CD45RO (OR 1.13), HLA-DR (OR 1.06), and Ki67 (1.22)
stats_prop <- getSigFeatures(fit$fit, type = 'prop',  prop = prop_logit_train,
                         mean = markerMeanCellType_train, clinicalData = clinicaldata_train, 
                         outcome = "Gensini_bin")

sigFeatures <- stats_prop$sigFeatures
stats <- stats_prop$stats
```

```{r}
# distribution plot
plotSigFeatures(stats = stats, sigFeatures = sigFeatures, outcome = "Gensini bin")

# boxplot
plotSigFeatures(stats = stats, sigFeatures = sigFeatures, outcome = "Gensini bin", 
                type = "boxplots") + 
    xlab("Gensini bin")
```
None of the non-zero marker means per cell type features are significant after adjusting for multiple testing. However, for the proportion model, both features returned by the model have statistically signficant differences between CAD status.

# Look at just the proportions
We can repeat the same experiment, but just looking at the proportions. This is done by fitting the overlapping group lasso model to the just logit of the cell type proportions. To do this, we just modify the training and testing inputs, as well as the grouping structure.

## Set up the training and testing data
```{r}
# training data
X_train <- prop_logit_train %>%
  as.matrix()

# testing data
X_test <- prop_logit_test %>%
  as.matrix()
```

## Set up the groups
The groups will just be from the hierarchical clustering
```{r}
groups <- generateGroups(tree = tree, nClust = 11, proportions = prop_logit_train,
                         means = markerMeanCellType_train, type = "prop")
print(groups)
```

## Fit Overlap model
Again fit the overlap model similarly as before
```{r}
groups <- lapply(groups, sort)
fit <- fitModel(xTrain = X_train, yTrain = y_train,
                groups = groups, penalty = "grLasso", seed = 1994)
```

## Predict on test data
```{r}
test_auc <- plotAUC(fit = fit$fit, xTest = X_test, yTest = y_test, 
                    title = "Test - Proportions AUC =")
test_auc$plot
```

## Visualise and assess  model outputs
```{r, fig.height=10, fig.width=12}
visualiseModelTree(fit = fit$fit, tree = tree, type = "cluster", 
                   trainingData = X_train, heatmap = FALSE)
```

Similarly, we can look at the distributions of these features and assess their statistical significance.
```{r, eval=T, fig.height=10, fig.width=16}
# CCR2 (OR 1.12), CCR4 (OR 1.08), CD38 and CD45RO (OR 1.13), HLA-DR (OR 1.06), and Ki67 (1.22)
stats_prop <- getSigFeatures(fit$fit, type = 'prop',  prop = prop_logit_train,
                         mean = markerMeanCellType_train, clinicalData = clinicaldata_train, 
                         outcome = "Gensini_bin")

sigFeatures <- stats_prop$sigFeatures
stats <- stats_prop$stats

```

```{r, fig.height=16, fig.width=20}
plotSigFeatures(stats = stats, sigFeatures = sigFeatures, outcome = "Gensini bin")

plotSigFeatures(stats = stats, sigFeatures = sigFeatures, outcome = "Gensini bin", type = "boxplots")
```

# Look at just the means
We can perform the same experiment for just the marker means per cell type. This is done by fitting the overlapping group lasso model to the just the marker means per cell type. To do this, we just modify the training and testing inputs, as well as the grouping structure.

## Set up the training and testing data
```{r}
# Combine all input data matrices
X_train <- markerMeanCellType_train %>%
  as.matrix()

X_test <- markerMeanCellType_test %>%
  as.matrix()
```

## Set up the grouping structure
Since this is the marker means per cell type, each feature will be in it's own group.
```{r}
groups <- generateGroups(tree = tree, nClust = 11, proportions = prop_logit_train,
                         means = markerMeanCellType_train, type = "mean")
head(groups)
```

## Fit Overlap model
Fit the model similar to before
```{r}
fit <- fitModel(xTrain = X_train, yTrain = y_train,
                groups = groups, penalty = "grLasso", seed = 1994)
```

## Predict on test data
```{r}
test_auc <- plotAUC(fit = fit$fit, xTest = X_test, yTest = y_test, 
                    title = "Test - Marker means per cell type AUC =")
test_auc$plot
```

## Visualise and assess  model outputs
Similar to the full model, we see similar marker means in cell types with non-zero coefficients.
```{r, fig.height=10, fig.width=12}
plotHeatmap(fit$fit, order = order, type = "cluster")
```

Lastly, we can look at the distributions of these features and assess their statistical significance.
```{r, eval=T}
# CCR2 (OR 1.12), CCR4 (OR 1.08), CD38 and CD45RO (OR 1.13), HLA-DR (OR 1.06), and Ki67 (1.22)
stats_mean <- getSigFeatures(fit$fit, type = 'mean',  
                         mean = markerMeanCellType_train, clinicalData = clinicaldata_train, 
                         outcome = "Gensini_bin")

sigFeatures <- stats_mean$sigFeatures
stats <- stats_mean$stats
```

```{r, fig.height=16, fig.width=20}
plotSigFeatures(stats = stats, sigFeatures = sigFeatures, outcome = "Gensini bin", )

plotSigFeatures(stats = stats, sigFeatures = sigFeatures, outcome = "Gensini bin", 
                type = "boxplots") + 
    xlab("Gensini bin")
```
As with the full model, none of the features have a statistically significant difference between CAD status after adjusting for multiple testing. We do however see a reduction in overall magnitudes of various p-values.

# Session Info
```{r}
sessionInfo()
```

